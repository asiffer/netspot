{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"advanced/","text":"Use netspot like a pro.","title":"Advanced"},{"location":"developer/","text":"First we recommend to look at netspot architecture . It will show you all the key elements of the tool as most of them can be extended.","title":"Developer"},{"location":"about/ids/","text":"If you are interested in network intrusion detection, you probably know that current defenses rely on rule-based intrusion detection systems (IDS) like Snort , Zeek or Suricata . They work very fine once you have the right rules but writing these rules is only possible when attacks are accurately known. That's where anomaly-based IDS come in! netspot is such an IDS. Many previous works have proposed such solutions but netspot is different because of its simplicity and above all its lack of ambition. Keep in mind that netspot won't flag all zero-day attacks, but it will find relevant anomalies on your network. This work has been published at IEEE TrustCom 2020 . Cite Siffer, A., Fouque, P. A., Termier, A., & Largouet, C. (2020, December). Netspot: a simple Intrusion Detection System with statistical learning. In 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom) (pp. 911-918). IEEE.","title":"Intrusion Detection System"},{"location":"about/performances/","text":"Performance is a key feature for network intrusion detection systems. The simplicity of netspot actually makes it fast . Comparison with other IDS If we compare netspot with Suricata (common rule-based IDS) and Kitsune (trendy anomaly-based IDS in the research area), we merely notice that netspot is far faster. The performances of Kitsune come from their original paper while some experiments have been performed on a capture file (provided by the authors of Kitsune) for Suricata and netspot . Desktop Raspberry Pi 3B+ Warning Suricata was not available on ARM platform during our tests Number of processors netspot tremendously uses goroutines. It brings much performance if your computer has several cores, so we may wonder the impact of the number of processors. For that, we show some runs we made on a desktop computer with 6 Intel(R) Core(TM) i5-8400 CPU @ 2.80GHz. The next results come from the analysis of a pcap file available on MAWILAB that basically stores 74M of packets. The graph below shows that even if you have few cores (or you want to limit their use), netspot remains efficient.","title":"Performances"},{"location":"about/performances/#comparison-with-other-ids","text":"If we compare netspot with Suricata (common rule-based IDS) and Kitsune (trendy anomaly-based IDS in the research area), we merely notice that netspot is far faster. The performances of Kitsune come from their original paper while some experiments have been performed on a capture file (provided by the authors of Kitsune) for Suricata and netspot .","title":"Comparison with other IDS"},{"location":"about/performances/#desktop","text":"","title":"Desktop"},{"location":"about/performances/#raspberry-pi-3b","text":"Warning Suricata was not available on ARM platform during our tests","title":"Raspberry Pi 3B+"},{"location":"about/performances/#number-of-processors","text":"netspot tremendously uses goroutines. It brings much performance if your computer has several cores, so we may wonder the impact of the number of processors. For that, we show some runs we made on a desktop computer with 6 Intel(R) Core(TM) i5-8400 CPU @ 2.80GHz. The next results come from the analysis of a pcap file available on MAWILAB that basically stores 74M of packets. The graph below shows that even if you have few cores (or you want to limit their use), netspot remains efficient.","title":"Number of processors"},{"location":"advanced/service/","text":"Even if it is not the main way to use netspot , it can run as a service, exposing a minimal REST API. netspot serve By default it listens at tcp://localhost:11000 , and you can visit http://localhost:11000 to look at the simple dashboard that displays the current config of netspot . Naturally, depending on the interface(s) you monitor, you would like to change the API endpoint not to pollute what netspot is analyzing. You can be changed it with the -e flag. For instance, you can consider a unix socket. netspot serve -e unix:///tmp/netspot.sock The server exposes few methods that allows to do roughly everything. Method Path Description GET /api/config Get the current config (JSON output) POST /api/config Change the config (JSON expected) POST /api/run Manage the status of netspot (start/stop) GET /api/stats Get the list of available statistics GET /api/devices Get the list of available interfaces In addition, a Go client is available in the api/client subpackage. go get -u github.com/asiffer/netspot/api/client","title":"Service"},{"location":"advanced/spot/","text":"SPOT is the core algorithm which monitors the network statistics. Its main strength is that it automatically provides a decision threshold based on the stream it monitors. The threshold provided by Spot has the nice following features: - Unsupervised computation (automatic, no label required) - Probabilistic meaning (quantile) - Stream ready computation (dynamic update, high throughput) In particular this work has been published in 2017 at KDD conference: Siffer, A., Fouque, P. A., Termier, A., & Largouet, C. (2017, August). Anomaly detection in streams with extreme value theory. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1067-1075). ACM In Netspot, you can modify the Spot parameters either at global scale (all statistics) or specifically (statistics-wise) Probabilistic threshold The parameter q is the main parameter of the Spot algorithm. It directly tunes the decision threshold $\\mathbb{P}(X>z_q) = q$ Model depth Calibration parameters Extra parameters","title":"Spot Algorithm"},{"location":"advanced/spot/#probabilistic-threshold","text":"The parameter q is the main parameter of the Spot algorithm. It directly tunes the decision threshold $\\mathbb{P}(X>z_q) = q$","title":"Probabilistic threshold"},{"location":"advanced/spot/#model-depth","text":"","title":"Model depth"},{"location":"advanced/spot/#calibration-parameters","text":"","title":"Calibration parameters"},{"location":"advanced/spot/#extra-parameters","text":"","title":"Extra parameters"},{"location":"developer/counter/","text":"Counters are elements of the miner . In the Go project layout, they must be located in the miner/counter sub-package. Every counter is related to a network layer (or directly to a packet). It means that it will access only on its layer specific field (we do not recommend to parse further layers in counters logic). The following layers are currently available: PKT (raw packet) ARP IPv4 ICMPv4 TCP UDP A counter must implement 3 simple functions given by the interface below. type BaseCtrInterface interface { Name () string // the name of the counter Value () uint64 // get back the value of the counter Reset () // method to reset the counter } In addition, it must implement a Process function related to the layers it depends on. For instance, an IPv4 counter must have a method with the following signature Process ( ip * layers . IPv4 ) You can have a look to the counters already implemented within netspot. As a general example, a counter is a simple file with the following content. // <layer>_<counter_name>.go package counters import ( \"github.com/google/gopacket/layers\" ) func init () { // The counter is registered at the very beginning Register ( & COUNTER { Counter : 0 }) } // COUNTER is a custom Counter I made type COUNTER struct { BaseCtr Counter uint64 } // Name returns the name of the counter func ( c * COUNTER ) Name () string { } // Value returns the current value of the counter func ( c * COUNTER ) Value () uint64 { } // Reset resets the counter func ( c * COUNTER ) Reset () { } // Process update the counter according to data it receives func ( c * COUNTER ) Process ( * layers . IPv4 ) { } Obviously, you can create more complex counters with additonal structures but you should keep in mind that its operation should be rather atomic. Atomic We recall that several goroutines are likely to call the Process function. It means that the internal counter value access is under high concurrency. For this purpose, you may have a look to the atomic/sync package. You can also have a look to this example .","title":"Counters"},{"location":"developer/exporter/","text":"TODO","title":"Exporting module"},{"location":"developer/stat/","text":"The statistics are the real values netspot monitors. In the same manner as the counters, you can implement your own statistic according to your needs. In particular, they must be located in the netspot/stats subpackage. Note The analyzer is in charge of managing statistics. When you implement a new statistic you don't have to take care of neither load/unload operations nor Spot algorithm configuration. Statistics are built on top of the counters. They use some counters to compute their own value. For example, the statistic R_SYN is computed as the ratio SYN / IP where SYN counts the number of SYN packets and IP counts the number of IP packets. The general interface of a statistic ( StatInterface ) is quite rich but only few functions must be implemented. // StatInterface gathers the common behavior of the statistics type StatInterface interface { Name () string Configure () error Requirement () [] string Compute ( ctrvalues [] uint64 ) float64 Update ( val float64 ) int UpProbability ( quantile float64 ) float64 DownProbability ( quantile float64 ) float64 GetThresholds () ( float64 , float64 ) Status () gospot . DSpotStatus } Actually, statistics must inherit from the the BaseStat object which already implements some functions of the StatInterface (especially all related to the Spot algorithm). // BaseStat is the basic structure which defines a statistic. It // embeds a string (its unique name) and a DSpot instance which monitors // itself. type BaseStat struct { name string dspot * gospot . DSpot // the spot instance } To define a new statistic you only need to inehrit from the previous structure and code the following 3 functions: type StatInterface interface { // ... Name () string Requirement () [] string Compute ( ctrvalues [] uint64 ) float64 // ... } Warning You have to register your statistic by calling the Register(stat StatInterface) in the init() function. Finally, your new statistic should be defined in a single file and may look like below. // my_stat.go package stats func init () { // The statistic is registered at the very beginning Register ( & STAT { BaseStat { name : \"...\" }}) } // STAT computes something to define type STAT struct { BaseStat // compose with BaseStat // you can add some other attributes } // Name returns the unique name of the stat func ( stat * STAT ) Name () string { return \"...\" } // Requirement returns the requested counters to compute the stat. // The order is important for the Compute() function func ( stat * STAT ) Requirement () [] string { } // Compute implements the way to compute the stat from the counters. // Counters are given in the same order as the Requirement() function func ( stat * STAT ) Compute ( ctrvalues [] uint64 ) float64 { }","title":"Statistics"},{"location":"developer/toolchain/","text":"netspot is distributed as statically-compiled binaries. The final executables notably embed the libpcap and musl libraries. Using a musl-based system (like alpine ) is then far easier to compile netspot . Obviously, you can dynamically link netspot to libpcap and the more common GNU libc, but the installation will require these dependencies on the target system. Here, we only detail the static build for different architectures. The dev/ includes some utilities to build netspot statically. Docker image First of all, you have to prepare the docker image to build netspot. A Dockerfile is provided in the dev/image/ subfolder. FROM golang:1.15.6-alpine # LIBPCAP ENV LIBPCAP_VERSION 1 .9.1 ENV LIBPCAP_DIR /libpcap # SYSTEM ENV PACKAGES \"nano bash linux-headers git flex bison wget make bluez-dev bluez\" ENV CGO_ENABLED 1 ENV CGO_LDFLAGS \"-L ${ LIBPCAP_DIR } /libpcap- ${ LIBPCAP_VERSION } \" ENV CGO_CFLAGS \"-O2 -I ${ LIBPCAP_DIR } /libpcap- ${ LIBPCAP_VERSION } \" # CROSS COMPILATION OPTIONS # see https://github.com/just-containers/musl-cross-make/releases/ ENV GCC_VERSION 9 .2.0 ENV MUSL_CROSS_MAKE_VERSION v15 ENV TARGET_ARCH \"x86_64-linux arm-linux aarch64-linux\" # SYSTEM RUN apk update ; apk add $PACKAGES # CROSS COMPILE TOOLCHAIN COPY install_toolchain.sh /install_toolchain.sh RUN /install_toolchain.sh # LIBPCAP COPY get_libpcap_sources.sh /get_libpcap_sources.sh RUN /get_libpcap_sources.sh CMD [ \"/bin/bash\" ] So you basically need to build this image (from the dev/image/ folder) docker build -t alpine-crossbuild-libpcap:latest . By default, this image will produce binaries for three different architectures: amd64 , arm and arm64 , but you can only select those you want by setting the TARGET_ARCH environment variable. docker build --build-arg TARGET_ARCH = \"x86_64-linux\" -t alpine-crossbuild-libpcap:latest . Warning Some environment variables must be set before building and not when a container starts (see the bash scripts below). The Dockerfile includes two bash scripts: - install_toolchain.sh , to download the cross-compilers - get_libpcap_sources.sh , to download the sources of libpcap You can inspect these files to check what environment variables they require. Compilation Now your image is ready, you just have to compile netspot . The dev/build/ folder gathers scripts for this purpose. The builder.sh script is the main file you have to run to compile netspot for a specific architecture builder.sh <ARCH> Warning The available ARCH depend on the previous image. By default you can choose between x86_64-linux arm-linux aarch64-linux This script must be executed within an instance of the previous image (container). So first, you have to run a container (here we suppose that netspot code lives in GOPATH/src/netspot but it is likely to be different accodint to your dev workflow): docker run --detach -it -v \" ${ GOPATH } /src/netspot:/go/src/netspot\" --name netspot-build alpine-crossbuild-libpcap:latest Then you can run the compilation for a specific architecture: docker exec netspot-build /go/src/netspot/dev/build/builder.sh <ARCH> Info The final binaries are located in the bin/ folder. The second script ( run.sh ) is an example that combine both steps. You can adapt it to your workflow.","title":"Developer toolchain"},{"location":"developer/toolchain/#docker-image","text":"First of all, you have to prepare the docker image to build netspot. A Dockerfile is provided in the dev/image/ subfolder. FROM golang:1.15.6-alpine # LIBPCAP ENV LIBPCAP_VERSION 1 .9.1 ENV LIBPCAP_DIR /libpcap # SYSTEM ENV PACKAGES \"nano bash linux-headers git flex bison wget make bluez-dev bluez\" ENV CGO_ENABLED 1 ENV CGO_LDFLAGS \"-L ${ LIBPCAP_DIR } /libpcap- ${ LIBPCAP_VERSION } \" ENV CGO_CFLAGS \"-O2 -I ${ LIBPCAP_DIR } /libpcap- ${ LIBPCAP_VERSION } \" # CROSS COMPILATION OPTIONS # see https://github.com/just-containers/musl-cross-make/releases/ ENV GCC_VERSION 9 .2.0 ENV MUSL_CROSS_MAKE_VERSION v15 ENV TARGET_ARCH \"x86_64-linux arm-linux aarch64-linux\" # SYSTEM RUN apk update ; apk add $PACKAGES # CROSS COMPILE TOOLCHAIN COPY install_toolchain.sh /install_toolchain.sh RUN /install_toolchain.sh # LIBPCAP COPY get_libpcap_sources.sh /get_libpcap_sources.sh RUN /get_libpcap_sources.sh CMD [ \"/bin/bash\" ] So you basically need to build this image (from the dev/image/ folder) docker build -t alpine-crossbuild-libpcap:latest . By default, this image will produce binaries for three different architectures: amd64 , arm and arm64 , but you can only select those you want by setting the TARGET_ARCH environment variable. docker build --build-arg TARGET_ARCH = \"x86_64-linux\" -t alpine-crossbuild-libpcap:latest . Warning Some environment variables must be set before building and not when a container starts (see the bash scripts below). The Dockerfile includes two bash scripts: - install_toolchain.sh , to download the cross-compilers - get_libpcap_sources.sh , to download the sources of libpcap You can inspect these files to check what environment variables they require.","title":"Docker image"},{"location":"developer/toolchain/#compilation","text":"Now your image is ready, you just have to compile netspot . The dev/build/ folder gathers scripts for this purpose. The builder.sh script is the main file you have to run to compile netspot for a specific architecture builder.sh <ARCH> Warning The available ARCH depend on the previous image. By default you can choose between x86_64-linux arm-linux aarch64-linux This script must be executed within an instance of the previous image (container). So first, you have to run a container (here we suppose that netspot code lives in GOPATH/src/netspot but it is likely to be different accodint to your dev workflow): docker run --detach -it -v \" ${ GOPATH } /src/netspot:/go/src/netspot\" --name netspot-build alpine-crossbuild-libpcap:latest Then you can run the compilation for a specific architecture: docker exec netspot-build /go/src/netspot/dev/build/builder.sh <ARCH> Info The final binaries are located in the bin/ folder. The second script ( run.sh ) is an example that combine both steps. You can adapt it to your workflow.","title":"Compilation"},{"location":"download/binaries/","text":"netspot is available on Linux platforms with different architectures: amd64 , arm and aarch64 . The Go ecosystem would allow to easily build netspot for Mac and Windows however it has not been tested, and that is probably not very relevant. You can download the latest binaries with the links below. netspot-2.1-amd64-linux-static netspot-2.1-arm-linux-static netspot-2.1-arm64-linux-static If you don't like to click, you can draw your inspiration from the following command. curl -Lo netspot 'https://github.com/asiffer/netspot/releases/download/v2.1/netspot-2.1-amd64-linux-static' chmod +x netspot Let us recall that you need some privilege to run netspot on network interfaces. So either you can run it as root (not recommended) or you can add the sniffing capability. sudo setcap cap_net_admin,cap_net_raw = eip ./netspot","title":"Binaries"},{"location":"download/docker/","text":"netspot is now available through a docker image, hosted on Github. You can have a look to the local registry to pull the image. Once you have pulled the image, you can run netspot interactively through: docker run -it --name netspot --cap-add NET_ADMIN --network host netspot:latest The capabilities NET_ADMIN allows to run netspot through a non-root user inside the container. In addition, here we use the host network because in practice you may want to deploy the IDS on host interfaces. You can tune the container a little through two environment variables: Environment variable Default value NETSPOT_ENDPOINT tcp://127.0.0.1:11000 NETSPOT_CONFIG_FILE /etc/netspot.toml Hence, you can load your config file (e.g. /tmp/config.toml ) by mounting it on the container. docker run -it --name netspot --cap-add NET_ADMIN --network host -v /tmp/config.toml:/etc/netspot.toml netspot:latest","title":"Docker"},{"location":"download/service/","text":"Work in progress...","title":"Systemd service"},{"location":"download/sources/","text":"You can get netspot by building it from sources. By default, it will dynamically link netspot to libpcap . At least, you will need libpcap-dev on your system (and a C compiler). Go magic Look at this magic! go install github.com/asiffer/netspot It installs netspot to $GOPATH/bin/netspot , so you must ensure it is in your path to use it directly from cli. Warning This method basically run the Go compiler on the sources. However it differs from the make build as the latter use additional \"release\" flags. In addition, the version output by the binary won't embed the git hash (only the major version). Classic make Warning As netspot uses new Go features, you should use a recent version of the Go compiler ( >=1.16 ). You can run the following script to update your version ( curl , sudo and tar commands are required): # Get latest version ID LAST_VERSION = $( curl -s https://golang.org/dl/ | grep -e 'go[0-9]*\\.[0-9]*\\.[0-9]*' -om 1 | sed 's/go//g' ) # prepare the dl link TAR = \"go ${ LAST_VERSION } .linux-amd64.tar.gz\" LINK = \"https://dl.google.com/go/ ${ TAR } \" # save the archive to /tmp curl -so \"/tmp/ ${ TAR } \" \" ${ LINK } \" # install it (you should remove the previous /usr/local/go folder) sudo tar -C /usr/local -xzf \"/tmp/ ${ TAR } \" # add to path export PATH = $PATH :/usr/local/go/bin # check version go version On Debian 10/Ubuntu 20.04 apt update apt install git make gcc libpcap-dev git clone https://github.com/asiffer/netspot.git cd netspot make It builds the binary on the bin/ folder. You can check the result: ./bin/netspot-2.1-amd64-linux --version Finally you can install it (as root) make install","title":"From sources"},{"location":"download/sources/#go-magic","text":"Look at this magic! go install github.com/asiffer/netspot It installs netspot to $GOPATH/bin/netspot , so you must ensure it is in your path to use it directly from cli. Warning This method basically run the Go compiler on the sources. However it differs from the make build as the latter use additional \"release\" flags. In addition, the version output by the binary won't embed the git hash (only the major version).","title":"Go magic"},{"location":"download/sources/#classic-make","text":"Warning As netspot uses new Go features, you should use a recent version of the Go compiler ( >=1.16 ). You can run the following script to update your version ( curl , sudo and tar commands are required): # Get latest version ID LAST_VERSION = $( curl -s https://golang.org/dl/ | grep -e 'go[0-9]*\\.[0-9]*\\.[0-9]*' -om 1 | sed 's/go//g' ) # prepare the dl link TAR = \"go ${ LAST_VERSION } .linux-amd64.tar.gz\" LINK = \"https://dl.google.com/go/ ${ TAR } \" # save the archive to /tmp curl -so \"/tmp/ ${ TAR } \" \" ${ LINK } \" # install it (you should remove the previous /usr/local/go folder) sudo tar -C /usr/local -xzf \"/tmp/ ${ TAR } \" # add to path export PATH = $PATH :/usr/local/go/bin # check version go version On Debian 10/Ubuntu 20.04 apt update apt install git make gcc libpcap-dev git clone https://github.com/asiffer/netspot.git cd netspot make It builds the binary on the bin/ folder. You can check the result: ./bin/netspot-2.1-amd64-linux --version Finally you can install it (as root) make install","title":"Classic make"},{"location":"get-started/architecture/","text":"The picture below details the internal structure of netspot . It aims to present how the IDS is designed and it is also likely to help both the user and the developer to better understand the tool. At the lowest level, netspot parse packets and increment some basic counters . This part is performed by the miner subpackage. The source can either be an network interface or a .pcap file (network capture). At a given frequency, counter values are retrieved so as to build statistics , this is the role of the analyzer . The statistics are the measures monitored by netspot . Every statistic embeds an instance of the SPOT algorithm to monitor itself. This algorithm learns the normal behaviour of the statistic and constantly updates its knowledge. When an abnormal value occurs, SPOT triggers an alarm. The analyzer forwards statistics values, SPOT thresholds and SPOT alarms to the exporter . This last component dispatch these information modules that binds to different backends (console, file, socket or InfluxDB database).","title":"Architecture"},{"location":"get-started/basic/","text":"One-liner Basically, you can run netspot on a network interface. In the example below, netspot monitors the PERF statistics (packet processing rate) on the eth0 interface. The computation period is 1s and the values are printed to the console ( -v ). netspot run -d eth0 -s PERF -p 1s -v You can also analyze a capture file with several statistics. netspot run -d file.pcap -s PERF -s R_SYN -s R_ARP -p 500ms -v Config file All the command-line options can be set in a config file (see the configuration section for more options): # netspot.toml [miner] device = \"~/file.pcap\" [analyzer] period = \"500ms\" stats = [\"PERF\", \"R_SYN\", \"R_ARP\"] [exporter.console] data = true netspot run --config netspot.toml Data output netspot outputs two things: the network statistics (namely the data , at every period ) the alarms (when a computed stat is abnormal) A stat record is a simple map STAT: value with a timestamp. When netspot learns, it also gathers the decision thresholds STAT_UP: upper_threshold and STAT_DOWN: lower_threshold . An alarm is generated once a statistics is beyond a threshold. It contains the value of the statistics and its probability to occur (the lower, the more abnormal). netspot can dispatch these two streams to different exporting modules. In the previous example, data (not the alarms) are sent to the console ( -v flag), but actually you can also send it to a file (see below), a socket or an influxdb database. # storing data to a file netspot run -d file.pcap -s PERF -s R_SYN -p 500ms -f /tmp/data.json There is not a short CLI flag for every module. In the general case, you have to use the module.submodule.option scheme (like in the config file): netspot run -d file.pcap -s PERF -s R_SYN -p 500ms --exporter.file.data /tmp/data.json Or you can set it in the config file: # netspot.toml [exporter.file] # Path to the file which will store the data. # The value can contain a '%s' which will be replaced by # the series name. data = \"/tmp/netspot_%s_data.json\" # Same as the data but for the alarms #alarm = \"/tmp/netspot_%s_alarm.json\"","title":"Basic usage"},{"location":"get-started/basic/#one-liner","text":"Basically, you can run netspot on a network interface. In the example below, netspot monitors the PERF statistics (packet processing rate) on the eth0 interface. The computation period is 1s and the values are printed to the console ( -v ). netspot run -d eth0 -s PERF -p 1s -v You can also analyze a capture file with several statistics. netspot run -d file.pcap -s PERF -s R_SYN -s R_ARP -p 500ms -v","title":"One-liner"},{"location":"get-started/basic/#config-file","text":"All the command-line options can be set in a config file (see the configuration section for more options): # netspot.toml [miner] device = \"~/file.pcap\" [analyzer] period = \"500ms\" stats = [\"PERF\", \"R_SYN\", \"R_ARP\"] [exporter.console] data = true netspot run --config netspot.toml","title":"Config file"},{"location":"get-started/basic/#data-output","text":"netspot outputs two things: the network statistics (namely the data , at every period ) the alarms (when a computed stat is abnormal) A stat record is a simple map STAT: value with a timestamp. When netspot learns, it also gathers the decision thresholds STAT_UP: upper_threshold and STAT_DOWN: lower_threshold . An alarm is generated once a statistics is beyond a threshold. It contains the value of the statistics and its probability to occur (the lower, the more abnormal). netspot can dispatch these two streams to different exporting modules. In the previous example, data (not the alarms) are sent to the console ( -v flag), but actually you can also send it to a file (see below), a socket or an influxdb database. # storing data to a file netspot run -d file.pcap -s PERF -s R_SYN -p 500ms -f /tmp/data.json There is not a short CLI flag for every module. In the general case, you have to use the module.submodule.option scheme (like in the config file): netspot run -d file.pcap -s PERF -s R_SYN -p 500ms --exporter.file.data /tmp/data.json Or you can set it in the config file: # netspot.toml [exporter.file] # Path to the file which will store the data. # The value can contain a '%s' which will be replaced by # the series name. data = \"/tmp/netspot_%s_data.json\" # Same as the data but for the alarms #alarm = \"/tmp/netspot_%s_alarm.json\"","title":"Data output"},{"location":"get-started/configuration/","text":"The configuration of netspot follows its architecture. There are three main sections: miner , analyzer and exporter . In addition, there is the spot section to configure the Spot algorithm. Miner The miner is responsible of packet parsing. So you give it all the necessary options to sniff either a network interface or a pcap file. The main parameter is device that defines the packets source . By default, it is set to \"any\" , meaning that it sniffs all the network interfaces. In addition you will find all the classical options you may pass to libpcap . Danger You must take care of the timeout parameter. By default, it is set to 0s , meaning that packets are directly sent to netspot. If this value is changed, you are likely to have a time lag in the statistics computation. # the Miner module manages the packets parsing # and the counters [miner] # name of the interface to listen or dump/pcap file path #device = \"any\" device = \"eth0\" #device = \"/tmp/file.pcap\" # interface only promiscuous = true snapshot_len = 65535 # instant mode timeout = \"0s\" Analyzer The analyzer computes the statistics. So we only need to set a list of statistics to monitor (parameter stats ) and to define the computation period . The period is relative to the device netspot sniffs. If the device is a pcap file, the source of time is the capture timestamps while it is the real time in the network interface case. Info You should set the period according to your device so as to make the stats computation relevant. For example, it is useless to set a very low value like 1ms if you have few packets a second. In practice, you should tune this parameter to ensure a rather low variance of the computed statistics (i.e. stable values). # The Analyzer module manages the statistics # and send data to the exporter [analyzer] # time between two statistics computations period = \"1s\" # stats to load at startup #stats = [\"AVG_PKT_SIZE\"] stats = [\"PERF\", \"R_SYN\", \"R_ACK\"] #stats = [ # \"PERF\", # \"R_ACK\", # \"R_ARP\", # \"R_DST_SRC\", # \"R_DST_SRC_PORT\", # \"R_ICMP\", # \"R_IP\", # \"R_SYN\", # \"TRAFFIC\" #] Exporter The exporter dispatches statistics and alarms to the desired backend. The exporter gathers several basic modules like the console , the file or the socket . In addition, netspot has also a module to send data to influxdb . For all the modules, you may notice that there are always two streams: data and alarms. You can activate them independently. Console The configuration of this module could not be easier. # The exporter print or send data according # to the loaded modules (and their options) # [exporter] [exporter.console] # print data to the console data = true # print alarms to the console alarm = true File The file module has a basic template. You can add %s in the output file: this will be replaced by the name of the running series. Data (and alarms) are stored as json records. [exporter.file] # Path to the file which will store the data. # The value can contain a '%s' which will be # replaced by the series name. data = \"/tmp/netspot_%s_data.json\" # Same as the data but for the alarms #alarm = \"/tmp/netspot_%s_alarm.json\" Socket The socket allows to send data in a \"generic way\", meaning without setting the protocol upon. In comparison to the above modules, you can add a tag into the sent data and change their format. Currently three formats are supported: csv , json and gob (golang binary format). [exporter.socket] # Path to the socket which will receive the data # The format is the following: <proto>://<address> data = \"unix:///tmp/netspot_data.socket\" # Path to the socket which will receive the alarms # The format is the following: <proto>://<address> alarm = \"unix:///tmp/netspot_alarm.socket\" # Additional tag when data are sent tag = \"netspot\" # Format of the data (accept csv, json or gob) format = \"json\" InfluxDB Finally, you can send netspot data to an InfluxDB database (version v1.x ). Classically, you need to define the endpoint, some credentials and the name of the database. Furthermore, for performance reasons, you can tune the batch_size (number of records to cache before sending). Like in the socket module, you can define an agent_name which is a kind of tag (it can be very convenient for InfluxDB). [exporter.influxdb1] #data = true #alarm = true #address = \"http://127.0.0.1:8086\" #database = \"netspot\" #username = \"netspot\" #password = \"netspot\" #batch_size = 5 #agent_name = \"local\" Spot The Spot section is specific to the configuration of the detection algorithm. To well understand the parameters, we advise to look at the Spot details . The parameters given in the [spot] section are the default parameter of the Spot instances monitoring the network statistics. # spot section manages the default spot parameters [spot] # depth = 50 # q = 1e-4 # n_init = 1000 # level = 0.98 # up = true # down = false # alert = true # bounded = true # max_excess = 200 However, you can define another Spot configuration for some statistics. You only need to create a section [spot.STAT_NAME] and overwrite some parameters. # you can add a specific section for a statistic # it overrides the default values [spot.R_SYN] q = 1e-5","title":"Configuration"},{"location":"get-started/configuration/#miner","text":"The miner is responsible of packet parsing. So you give it all the necessary options to sniff either a network interface or a pcap file. The main parameter is device that defines the packets source . By default, it is set to \"any\" , meaning that it sniffs all the network interfaces. In addition you will find all the classical options you may pass to libpcap . Danger You must take care of the timeout parameter. By default, it is set to 0s , meaning that packets are directly sent to netspot. If this value is changed, you are likely to have a time lag in the statistics computation. # the Miner module manages the packets parsing # and the counters [miner] # name of the interface to listen or dump/pcap file path #device = \"any\" device = \"eth0\" #device = \"/tmp/file.pcap\" # interface only promiscuous = true snapshot_len = 65535 # instant mode timeout = \"0s\"","title":"Miner"},{"location":"get-started/configuration/#analyzer","text":"The analyzer computes the statistics. So we only need to set a list of statistics to monitor (parameter stats ) and to define the computation period . The period is relative to the device netspot sniffs. If the device is a pcap file, the source of time is the capture timestamps while it is the real time in the network interface case. Info You should set the period according to your device so as to make the stats computation relevant. For example, it is useless to set a very low value like 1ms if you have few packets a second. In practice, you should tune this parameter to ensure a rather low variance of the computed statistics (i.e. stable values). # The Analyzer module manages the statistics # and send data to the exporter [analyzer] # time between two statistics computations period = \"1s\" # stats to load at startup #stats = [\"AVG_PKT_SIZE\"] stats = [\"PERF\", \"R_SYN\", \"R_ACK\"] #stats = [ # \"PERF\", # \"R_ACK\", # \"R_ARP\", # \"R_DST_SRC\", # \"R_DST_SRC_PORT\", # \"R_ICMP\", # \"R_IP\", # \"R_SYN\", # \"TRAFFIC\" #]","title":"Analyzer"},{"location":"get-started/configuration/#exporter","text":"The exporter dispatches statistics and alarms to the desired backend. The exporter gathers several basic modules like the console , the file or the socket . In addition, netspot has also a module to send data to influxdb . For all the modules, you may notice that there are always two streams: data and alarms. You can activate them independently.","title":"Exporter"},{"location":"get-started/configuration/#console","text":"The configuration of this module could not be easier. # The exporter print or send data according # to the loaded modules (and their options) # [exporter] [exporter.console] # print data to the console data = true # print alarms to the console alarm = true","title":"Console"},{"location":"get-started/configuration/#file","text":"The file module has a basic template. You can add %s in the output file: this will be replaced by the name of the running series. Data (and alarms) are stored as json records. [exporter.file] # Path to the file which will store the data. # The value can contain a '%s' which will be # replaced by the series name. data = \"/tmp/netspot_%s_data.json\" # Same as the data but for the alarms #alarm = \"/tmp/netspot_%s_alarm.json\"","title":"File"},{"location":"get-started/configuration/#socket","text":"The socket allows to send data in a \"generic way\", meaning without setting the protocol upon. In comparison to the above modules, you can add a tag into the sent data and change their format. Currently three formats are supported: csv , json and gob (golang binary format). [exporter.socket] # Path to the socket which will receive the data # The format is the following: <proto>://<address> data = \"unix:///tmp/netspot_data.socket\" # Path to the socket which will receive the alarms # The format is the following: <proto>://<address> alarm = \"unix:///tmp/netspot_alarm.socket\" # Additional tag when data are sent tag = \"netspot\" # Format of the data (accept csv, json or gob) format = \"json\"","title":"Socket"},{"location":"get-started/configuration/#influxdb","text":"Finally, you can send netspot data to an InfluxDB database (version v1.x ). Classically, you need to define the endpoint, some credentials and the name of the database. Furthermore, for performance reasons, you can tune the batch_size (number of records to cache before sending). Like in the socket module, you can define an agent_name which is a kind of tag (it can be very convenient for InfluxDB). [exporter.influxdb1] #data = true #alarm = true #address = \"http://127.0.0.1:8086\" #database = \"netspot\" #username = \"netspot\" #password = \"netspot\" #batch_size = 5 #agent_name = \"local\"","title":"InfluxDB"},{"location":"get-started/configuration/#spot","text":"The Spot section is specific to the configuration of the detection algorithm. To well understand the parameters, we advise to look at the Spot details . The parameters given in the [spot] section are the default parameter of the Spot instances monitoring the network statistics. # spot section manages the default spot parameters [spot] # depth = 50 # q = 1e-4 # n_init = 1000 # level = 0.98 # up = true # down = false # alert = true # bounded = true # max_excess = 200 However, you can define another Spot configuration for some statistics. You only need to create a section [spot.STAT_NAME] and overwrite some parameters. # you can add a specific section for a statistic # it overrides the default values [spot.R_SYN] q = 1e-5","title":"Spot"}]}